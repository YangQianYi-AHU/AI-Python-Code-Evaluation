# AI-Python-Code-Evaluation
This repository evaluates AI models' Python code generation abilities across tasks from basic syntax to complex algorithms. It compares models based on correctness, efficiency, and code quality.
### Project Overview: Evaluating AI Code Generation in Python

**Objective:**
The primary objective of this project is to systematically evaluate and compare the code generation abilities of various AI models in Python. By doing so, we aim to identify the strengths and limitations of each model across a spectrum of coding tasks, ranging from basic syntax and logic to more complex algorithm implementation, data manipulation, and machine learning.

**Scope:**
This project will involve generating Python scripts using different AI models based on a series of progressively challenging tasks. Each AI model will be provided with a detailed prompt, and the generated code will be analyzed for correctness, efficiency, readability, and the ability to handle edge cases.

**Tasks:**
The project is structured around ten tasks, designed to assess various aspects of programming:

1. **Hello, World!**: Basic syntax and output.
2. **Simple Calculator**: User input handling, arithmetic operations, and conditionals.
3. **Palindrome Checker**: String manipulation and logic.
4. **Fibonacci Sequence**: Sequence generation, loops, and recursion.
5. **Prime Number Checker**: Efficient checking of prime numbers using loops and conditionals.
6. **Sorting Algorithm (Bubble Sort)**: Implementation of a classic algorithm, focusing on loops and list manipulation.
7. **Binary Search**: Search algorithms, including both iterative and recursive approaches.
8. **File I/O Operations**: Handling files, counting data, and saving results.
9. **Web Scraping**: Usage of external libraries, web scraping, and data storage.
10. **Basic Machine Learning Model**: Building and training a linear regression model, involving data manipulation and library usage.

**Methodology:**
- **Prompting:** Each AI model will be given a specific, well-defined prompt for the tasks listed above.
- **Code Generation:** The AI-generated code will be evaluated for correctness, efficiency, and readability. This includes checking whether the code fulfills the task requirements and how well it handles typical edge cases.
- **Comparison:** The performance of different AI models will be compared based on metrics like execution correctness, code quality, adherence to Python best practices, and the ability to produce efficient solutions.

**Outcomes:**
The outcomes of this project will include a comprehensive analysis of the capabilities of various AI models in Python code generation. The results will be valuable for understanding the current state of AI in programming tasks, guiding further development of AI tools for developers, and informing decisions on which AI models are best suited for specific programming tasks.

**Potential Impact:**
By providing insights into the strengths and weaknesses of AI in generating Python code, this project could help in optimizing the use of AI in software development, improving AI models for specific coding tasks, and enhancing the efficiency and accuracy of AI-assisted programming.

**Future Work:**
Following the completion of this project, the findings will be published in research papers, contributing to the broader understanding of AI's role in software engineering. Further research could explore more complex tasks, integration of AI-generated code with existing codebases, and the development of AI models specifically tailored for programming.
